---
layout: post
title:  "timekit: Time Series Forecast Applications Using Data Mining"
author: "Matt Dancho"
categories: [Code-Tools]
tags: [R-Project, R, timekit, tidyquant, padr]
image: timekit-0-2-0.png
---




The `timekit` package contains a collection of tools for working with time series in R. There's a number of benefits. One of the biggest is the ability to use a time series signature to predict future values (forecast) through data mining techniques. While this post is geared toward exposing the user to the `timektit` package, there are examples showing the power of data mining a time series as well as how to work with time series in general. A number of `timekit` functions will be discussed and implemented in the post. The first group of functions works with the time series index, and these include functions `tk_index()`, `tk_get_timeseries_signature()`, `tk_augment_timeseries_signature()` and `tk_get_timeseries_summary()`. We'll spend the bulk of this post introducing you to these. The next function deals with creating a future time series from and existing index, `tk_make_future_timeseries()`. The last set of functions deal with coercion to and from the major time series classes in R, `tk_tbl()`, `tk_xts()`, `tk_zoo()` (and `tk_zooreg()`), and `tk_ts()`.  



# Benefits

So, why another time series package? The short answer is because it helps with data mining, communication between time series objects, and facilitating accurate future time series. The long answer is slightly more complicated, and I will attempt to explain.

##### Time Series Signature

The first reason and arguably the most important reason is the idea that there is a large amount of information stored inside a simple yet complex __time index__ that is very useful in for modeling and data mining. The time index is the collection of time-based values that define _when_ each observation occurred. Consider the timestamp "2016-01-01 00:00:00". This contains a wealth of information related to the observation including year, month, day, hour, minute and second. We can even extract more information including half, quarter, week of year, day of year, and so on with little effort. Next is the concept of the __frequency__ (or periodicity or scale), which is the amount of time between multiple observations. From this time difference we can get even more information such as the periodicity of the data, whether the observations are regular or irregularly spaced, and even which observations are frequently missing. By my count, there's at least 20+ features that can be retrieved from a simple timestamp. The important concept is that these features can exploded (or broken out) into what I'm calling the __time series signature__, which is nothing more than a decomposition of the unique features related to time index values. This data is very useful as it can be summarized, modeled, mined, sliced and diced, etc. As and example of the power of the signature, we can generate a prediction using data mining techniques such as this (see alcohol sales example later).

![Forecasting Alcohol Sales](/assets/timekit-0-2-0.png)

##### Prediction and Forecast Accuracy

The second reason is that often we want to make predictions into the future. There's a number of packages such as `forecast` and `prophet` that already specialize in this. For `forecast` the future dates can be incorrect especially for daily data. A regular numeric system doesn't contain true dates and a sequential system results in inaccuracy with respect to irregular dates. For prophet, the mechanism to compute holidays and missing days is internal to the `predict()` method, and therefore the a method specific to creating future dates is needed. Two types of days cause problems: those that are regularly skipped and irregularly skipped. The regularly skipped days (such as weekends or sometimes companies get to take every other friday off) need to be factored into the future date sequence. The irregularly skipped days (think holidays) cause issues as well, and these suffer the additional problem as they can be difficult (but not impossible) to predict. 

##### Communication and Coercion Between Time-Based Object Classes

The third reason is that the R object structures that contain time-based information are difficult use together. My first attempt was born in `tidyquant` where I created the `as_tibble()` and `as_xts()` functions to coerce (convert) back and forth. I was naive in this attempt because the problem is larger: we have `zoo`, `ts` and many other packages that work with time-based information. The `xts` and `zoo` packages solved part of the problem, but there's two issues that persist. First, the time-based tibble ("tidy" data frame with class `tbl`) does not communicate well with the rest of the group. Coercing to `xts`, `zoo` and `ts` objects can result in a lot of issues especially when coercion rules for homogenous object classes take over. Weird things can happen such as turning numeric data into character or converting date to numeric without warning. Further, each coercion method (`as_tibble`, `as.xts`, `as.zoo`, `as.ts`) has its own nuances that are inconsistent. Second, some classes like `ts` do not use a time-based index, but rather use a regularized numeric-based index. Without maintaining the time-based index, we can never go back to the original data structure, whether it is `tbl`, `xts`, `zoo`, etc.

##### Enter timekit

The `timekit` package solves each of these issues. It includes functions to create a time series signature and a time series summary from a sequence of dates. It includes methods to accurately generate future time series index values, which is especially important for daily data. It provides consistent coercion methods that prevent inadvertent class coercion issues resulting from homogenous object structures and that maximize time-based index retention for regularized data structures.  

# Test Driving timekit

Let's take `timekit` for a test drive. We'll be using a few other packages in the process to help with the examples. First, install `timekit`.


{% highlight r %}
install.packages("timekit")
{% endhighlight %}

Next, load these packages:


{% highlight r %}
library(tidyquant)
library(padr)
library(timekit)
{% endhighlight %}

## Example 1: Modeling Dates in a Series

This example is intended to expose potential users to several functions in `timekit`. We'll use an existing time series index to model future dates using the __time series signature__. First, start with the `FANG` data set from the `tidyquant` package. Filter to get just the FB stock prices, and select the "date" and "volume" columns. This is a typical time series data structure. A time-based tibble with a "date" column and a features column ("volume" in this case). 


{% highlight r %}
FB_tbl <- FANG %>%
    filter(symbol == "FB") %>%
    select(date, volume)
FB_tbl
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,008 × 2
##          date    volume
##        <date>     <dbl>
## 1  2013-01-02  69846400
## 2  2013-01-03  63140600
## 3  2013-01-04  72715400
## 4  2013-01-07  83781800
## 5  2013-01-08  45871300
## 6  2013-01-09 104787700
## 7  2013-01-10  95316400
## 8  2013-01-11  89598000
## 9  2013-01-14  98892800
## 10 2013-01-15 173242600
## # ... with 998 more rows
{% endhighlight %}

##### Extract and Review the Index

Extract the index using `tk_index()`, which collects the time-based index values as a vector of the native class (in this case `date` class). 


{% highlight r %}
# Extract index values
idx <- tk_index(FB_tbl)
{% endhighlight %}

The series is irregular. We can tell this by reviewing the timeseries summary using `tk_get_timeseries_summary()`. The output is a single-row tibble. The first half of the columns contain a general summary.


{% highlight r %}
# Review timeseries summary; columns 1-6 contain general summary
tk_get_timeseries_summary(idx)[1:6]
{% endhighlight %}



{% highlight text %}
## # A tibble: 1 × 6
##   n.obs      start        end units scale tzone
##   <int>     <date>     <date> <chr> <chr> <chr>
## 1  1008 2013-01-02 2016-12-30  days   day   UTC
{% endhighlight %}

The second half (columns 7 through 12) containing the difference summary measured in seconds. Because the median is different from the mean, we can tell that the series is irregular meaning dates are missing. 


{% highlight r %}
# Review timeseries summary; columns 7-12 contain diff summary
tk_get_timeseries_summary(idx)[7:12]
{% endhighlight %}



{% highlight text %}
## # A tibble: 1 × 6
##   diff.minimum diff.q1 diff.median diff.mean diff.q3 diff.maximum
##          <dbl>   <dbl>       <dbl>     <dbl>   <dbl>        <dbl>
## 1        86400   86400       86400    125100   86400       345600
{% endhighlight %}

##### Preparing for Modeling

For modeling purposes we need to fill in the dates and relate the present and missing dates to 1's and 0's, respectively. To do this, first create a series of 1's using repeat. The 1's indicate that the values are present in the index. Then create a tibble of dates that are known to be present in the existing index using the `tibble()` function. The output `date_tbl` will be used to define which dates are present in the series.


{% highlight r %}
# Create dates with 1's indicating present
y <- rep(1, length(idx))
date_tbl <- tibble(idx, y)
date_tbl
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,008 × 2
##           idx     y
##        <date> <dbl>
## 1  2013-01-02     1
## 2  2013-01-03     1
## 3  2013-01-04     1
## 4  2013-01-07     1
## 5  2013-01-08     1
## 6  2013-01-09     1
## 7  2013-01-10     1
## 8  2013-01-11     1
## 9  2013-01-14     1
## 10 2013-01-15     1
## # ... with 998 more rows
{% endhighlight %}

We can add the missing dates using `pad()` from the `padr()` package. We can also use the `fill_by_value()` function to replace `NA` values with zero. The resulting data frame is now regularized with zeros and ones indicating missing and present dates.


{% highlight r %}
# Padding = adding missing dates; missing = 0
date_tbl_pad <- date_tbl %>%
    pad() %>%
    fill_by_value(y, value = 0)
date_tbl_pad
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,459 × 2
##           idx     y
##        <date> <dbl>
## 1  2013-01-02     1
## 2  2013-01-03     1
## 3  2013-01-04     1
## 4  2013-01-05     0
## 5  2013-01-06     0
## 6  2013-01-07     1
## 7  2013-01-08     1
## 8  2013-01-09     1
## 9  2013-01-10     1
## 10 2013-01-11     1
## # ... with 1,449 more rows
{% endhighlight %}

##### Modeling the Time Series

Now for the fun part. We can use the `tk_augment_timeseries_signature()` function to add the time series signature to the right side of a data frame (or `xts` or `zoo` object). We now have the index, y values, and all of the signature features in one data frame. 


{% highlight r %}
# Augmenting = Adding columns to a data frame
date_tbl_signature <- tk_augment_timeseries_signature(date_tbl_pad)
date_tbl_signature
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,459 × 24
##           idx     y  index.num  diff  year  half quarter month
##        <date> <dbl>      <int> <int> <int> <int>   <int> <int>
## 1  2013-01-02     1 1357084800    NA  2013     1       1     1
## 2  2013-01-03     1 1357171200 86400  2013     1       1     1
## 3  2013-01-04     1 1357257600 86400  2013     1       1     1
## 4  2013-01-05     0 1357344000 86400  2013     1       1     1
## 5  2013-01-06     0 1357430400 86400  2013     1       1     1
## 6  2013-01-07     1 1357516800 86400  2013     1       1     1
## 7  2013-01-08     1 1357603200 86400  2013     1       1     1
## 8  2013-01-09     1 1357689600 86400  2013     1       1     1
## 9  2013-01-10     1 1357776000 86400  2013     1       1     1
## 10 2013-01-11     1 1357862400 86400  2013     1       1     1
## # ... with 1,449 more rows, and 16 more variables: month.xts <int>,
## #   month.lbl <ord>, day <int>, hour <int>, minute <int>,
## #   second <int>, wday <int>, wday.xts <int>, wday.lbl <ord>,
## #   mday <int>, yday <int>, week <int>, week.iso <int>, week2 <int>,
## #   week3 <int>, week4 <int>
{% endhighlight %}

We can perform a logistic regression on the time series signature features. Use the `glm()` function with `family = binomial(link = 'logit')` and `data = data_tbl_signature[,-1]`. We drop the "index" column which adds no value to the logistic regression.    


{% highlight r %}
# Modeling the signature using logistic regression
fit <- glm(y ~ ., family = binomial(link = 'logit'), data = date_tbl_signature[,-1])
summary(fit)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## glm(formula = y ~ ., family = binomial(link = "logit"), data = date_tbl_signature[, 
##     -1])
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -3.08885   0.00000   0.00003   0.22219   0.78404  
## 
## Coefficients: (12 not defined because of singularities)
##                Estimate Std. Error z value Pr(>|z|)  
## (Intercept)  -9.158e+04  6.477e+05  -0.141   0.8876  
## index.num    -1.469e-06  1.042e-05  -0.141   0.8879  
## diff                 NA         NA      NA       NA  
## year          4.650e+01  3.288e+02   0.141   0.8875  
## half         -8.368e+01  8.038e+03  -0.010   0.9917  
## quarter       3.345e+01  3.221e+03   0.010   0.9917  
## month         1.221e+01  4.754e+02   0.026   0.9795  
## month.xts            NA         NA      NA       NA  
## month.lbl.L          NA         NA      NA       NA  
## month.lbl.Q  -1.046e+01  1.132e+03  -0.009   0.9926  
## month.lbl.C  -4.599e+01  4.380e+03  -0.010   0.9916  
## month.lbl^4   7.698e-01  1.181e+03   0.001   0.9995  
## month.lbl^5   4.366e+01  4.281e+03   0.010   0.9919  
## month.lbl^6  -1.227e+00  8.139e+02  -0.002   0.9988  
## month.lbl^7  -1.828e+01  2.405e+03  -0.008   0.9939  
## month.lbl^8   8.107e+00  1.503e+03   0.005   0.9957  
## month.lbl^9          NA         NA      NA       NA  
## month.lbl^10  8.032e+00  1.387e+03   0.006   0.9954  
## month.lbl^11         NA         NA      NA       NA  
## day           3.669e-01  9.880e-01   0.371   0.7103  
## hour                 NA         NA      NA       NA  
## minute               NA         NA      NA       NA  
## second               NA         NA      NA       NA  
## wday         -5.316e-01  4.704e+02  -0.001   0.9991  
## wday.xts             NA         NA      NA       NA  
## wday.lbl.L           NA         NA      NA       NA  
## wday.lbl.Q   -5.372e+01  3.132e+03  -0.017   0.9863  
## wday.lbl.C    7.086e+00  2.111e+03   0.003   0.9973  
## wday.lbl^4   -1.784e+01  1.343e+03  -0.013   0.9894  
## wday.lbl^5   -1.089e+01  1.649e+03  -0.007   0.9947  
## wday.lbl^6    5.915e+00  1.442e+03   0.004   0.9967  
## mday                 NA         NA      NA       NA  
## yday                 NA         NA      NA       NA  
## week         -1.513e+00  1.157e+00  -1.307   0.1913  
## week.iso     -2.739e-01  9.949e-01  -0.275   0.7831  
## week2        -5.668e-01  4.288e-01  -1.322   0.1862  
## week3         4.934e-01  2.441e-01   2.021   0.0433 *
## week4         3.150e-02  1.873e-01   0.168   0.8665  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1803.73  on 1457  degrees of freedom
## Residual deviance:  232.55  on 1432  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 284.55
## 
## Number of Fisher Scoring iterations: 21
{% endhighlight %}

##### Predicting a Future Date Sequence

Now that we have a model, we want to see how it performs. Make a sequence of future dates, `idx_future` using `tk_make_future_timeseries()`. Set `n_future = 20` to indicate 20 observations, and set `inspect_weekdays = FALSE` so the weekday prediction algorithm internal to the function is not run. This returns a sequence of consecutive days from the last date of the existing index, `idx`. We convert to a time series signature using `tk_get_timeseries_signature(idx_future)`. 


{% highlight r %}
idx_future <- tk_make_future_timeseries(idx, n_future = 20, inspect_weekdays = FALSE)
new_data   <- tk_get_timeseries_signature(idx_future)
new_data
{% endhighlight %}



{% highlight text %}
## # A tibble: 20 × 23
##         index  index.num  diff  year  half quarter month month.xts
##        <date>      <int> <int> <int> <int>   <int> <int>     <int>
## 1  2016-12-31 1483142400    NA  2016     2       4    12        11
## 2  2017-01-01 1483228800 86400  2017     1       1     1         0
## 3  2017-01-02 1483315200 86400  2017     1       1     1         0
## 4  2017-01-03 1483401600 86400  2017     1       1     1         0
## 5  2017-01-04 1483488000 86400  2017     1       1     1         0
## 6  2017-01-05 1483574400 86400  2017     1       1     1         0
## 7  2017-01-06 1483660800 86400  2017     1       1     1         0
## 8  2017-01-07 1483747200 86400  2017     1       1     1         0
## 9  2017-01-08 1483833600 86400  2017     1       1     1         0
## 10 2017-01-09 1483920000 86400  2017     1       1     1         0
## 11 2017-01-10 1484006400 86400  2017     1       1     1         0
## 12 2017-01-11 1484092800 86400  2017     1       1     1         0
## 13 2017-01-12 1484179200 86400  2017     1       1     1         0
## 14 2017-01-13 1484265600 86400  2017     1       1     1         0
## 15 2017-01-14 1484352000 86400  2017     1       1     1         0
## 16 2017-01-15 1484438400 86400  2017     1       1     1         0
## 17 2017-01-16 1484524800 86400  2017     1       1     1         0
## 18 2017-01-17 1484611200 86400  2017     1       1     1         0
## 19 2017-01-18 1484697600 86400  2017     1       1     1         0
## 20 2017-01-19 1484784000 86400  2017     1       1     1         0
## # ... with 15 more variables: month.lbl <ord>, day <int>, hour <int>,
## #   minute <int>, second <int>, wday <int>, wday.xts <int>,
## #   wday.lbl <ord>, mday <int>, yday <int>, week <int>,
## #   week.iso <int>, week2 <int>, week3 <int>, week4 <int>
{% endhighlight %}

Use the `predict()` function to apply the model `fit` to the new data `new_data`.  


{% highlight r %}
pred <- predict(fit, newdata = new_data, type = "response")
{% endhighlight %}

Let's see how the predictions performed. In a logistic regression, typically the acceptance criteria is 0.5. This means any value above 0.5 is predicted to be a date in the sequence and below 0.5 is predicted to be out of the sequence. We can chart using `ggplot()`. 


{% highlight r %}
new_data_pred <- tibble(date = idx_future, yhat = pred) 
new_data_pred %>%
    ggplot(aes(x = date, y = yhat)) +
    geom_point(color = palette_light()[[1]]) +
    geom_line(color = palette_light()[[1]]) +
    geom_hline(yintercept = 0.5, color = palette_light()[[2]]) +
    scale_x_date(date_breaks = "1 day", date_labels = "%a, %b %d-%y") +
    labs(title = "Predicting the Next Dates in a Series",
         x = "", subtitle = "Algorithm indicates weekends should be omitted") +
    theme_tq(base_size = 12) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
{% endhighlight %}

![plot of chunk unnamed-chunk-13](/figure/source/2017-5-2-timekit-0-2-0/unnamed-chunk-13-1.png)

The algorithm is clearly indicating that Saturdays and Sundays are not present in the data. Also worth noting is that our algorithm is indicating that January 2nd and January 16th tend to be less likely than the other dates. This is because these are US holidays that during which the market tends not to trade. However, because of the inconsistency in each year, the 0.5 cutoff would accept these days. 

## Example 2: Expedited Method to Make a Future Time Series

The previous example was a nice excercise to show off some new functions, namely `tk_get_timeseries_signature`, `tk_augument_timeseries_signature()` and `tk_get_timeseries_summary()`, and how they can be used for modeling and data mining. In practice we have another function to help us produce a future timeseries: `tk_make_future_timeseries()`. We already saw this function produce sequential future dates with the argument `inspect_weekdays = FALSE`. If we toggle to `TRUE`, we get the expected result. Let's see how it works.

First, make a future time series index from the existing index. We will only pass in 12 future values rather than 20 previously (we will see why shortly). Add `skip_values` for the holidays by making a sequence of dates with the holidays that should be skipped. Set `inspect_weekdays = TRUE`. 


{% highlight r %}
# Make future index
holidays <- c("2017-01-02", "2017-01-16") %>% ymd()
idx_future_2 <- tk_make_future_timeseries(idx, 
                                          n_future = 12, 
                                          skip_values = holidays,
                                          inspect_weekdays = TRUE) 
{% endhighlight %}

Build a tibble of predictions using the future index as the dates and a repeated sequence of 1's as the positive prediction. Then use `pad()` to fill the missing dates in the sequence, and `fill_by_value()` to replace `NA` values with zeros. 


{% highlight r %}
# Use pad() to fill in missing values
new_data_pred_2 <- tibble(date = idx_future_2, yhat = rep(1, length(idx_future_2))) %>%
    pad() %>%
    fill_by_value(yhat, value = 0)
{% endhighlight %}

Finally, plot the future date sequence. The weekends were automatically removed by the `inspect_weekdays = TRUE` algorithm. The irregularly sequenced holidays were removed manually by the `skip_values` argument.  


{% highlight r %}
# Plot using ggplot()
new_data_pred_2 %>%
    ggplot(aes(x = date, y = yhat)) +
    geom_point(color = palette_light()[[1]]) +
    geom_line(color = palette_light()[[1]]) +
    geom_hline(yintercept = 0.5, color = palette_light()[[2]]) +
    scale_x_date(date_breaks = "1 day", date_labels = "%a, %b %d-%y") +
    labs(title = "Auto Predicting Future Dates",
         x = "", subtitle = "The frequently missed dates (weekends) are automatically removed from the series") +
    theme_tq(base_size = 12) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
{% endhighlight %}

![plot of chunk unnamed-chunk-16](/figure/source/2017-5-2-timekit-0-2-0/unnamed-chunk-16-1.png)

The `tk_make_future_timeseries()` function expedites the process of determining future dates. The internal algorithm specifically targets weekdays on weekly, bi-weekly, tri-weekly, and quad-weekly frequencies to determine which frequently missing weekdays should be removed. 

## Example 3: Predicting Daily Volume for FB

__Now that we know how to get a future index, we can model future data using data mining techniques!__ Let's model the daily FB volume to see how we can apply an algorithm to a time series signature to predict future daily volume. 

First, split the data into two sets, one for training and one for comparing the actual output to our predictions.


{% highlight r %}
# Everything before 2016 will be used for training (2013-2015 data)
train <- FB_tbl %>%
    filter(date < ymd("2016-01-01")) 
# Everything in 2016 will be used for comparing the output
actual_future <- FB_tbl %>%
    filter(date >= ymd("2016-01-01"))
{% endhighlight %}

Next, augment the time series signature to the training set using `tk_augment_timeseries_signature()`. This gives us the time series signature data to mine. 


{% highlight r %}
train <- tk_augment_timeseries_signature(train)
train
{% endhighlight %}



{% highlight text %}
## # A tibble: 756 × 24
##          date    volume  index.num   diff  year  half quarter month
##        <date>     <dbl>      <int>  <int> <int> <int>   <int> <int>
## 1  2013-01-02  69846400 1357084800     NA  2013     1       1     1
## 2  2013-01-03  63140600 1357171200  86400  2013     1       1     1
## 3  2013-01-04  72715400 1357257600  86400  2013     1       1     1
## 4  2013-01-07  83781800 1357516800 259200  2013     1       1     1
## 5  2013-01-08  45871300 1357603200  86400  2013     1       1     1
## 6  2013-01-09 104787700 1357689600  86400  2013     1       1     1
## 7  2013-01-10  95316400 1357776000  86400  2013     1       1     1
## 8  2013-01-11  89598000 1357862400  86400  2013     1       1     1
## 9  2013-01-14  98892800 1358121600 259200  2013     1       1     1
## 10 2013-01-15 173242600 1358208000  86400  2013     1       1     1
## # ... with 746 more rows, and 16 more variables: month.xts <int>,
## #   month.lbl <ord>, day <int>, hour <int>, minute <int>,
## #   second <int>, wday <int>, wday.xts <int>, wday.lbl <ord>,
## #   mday <int>, yday <int>, week <int>, week.iso <int>, week2 <int>,
## #   week3 <int>, week4 <int>
{% endhighlight %}

Next, model the data using a regression. We are going to use the `lm()` function to model volume using the time series signature.


{% highlight r %}
fit_lm <- lm(volume ~ ., data = train[,-1])
summary(fit_lm)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = volume ~ ., data = train[, -1])
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -56182422 -14721686  -3529158   9826043 289760015 
## 
## Coefficients: (12 not defined because of singularities)
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.986e+11  4.109e+11   0.727   0.4677    
## index.num     4.266e+00  6.607e+00   0.646   0.5187    
## diff         -4.755e+01  2.987e+01  -1.592   0.1118    
## year         -1.512e+08  2.086e+08  -0.725   0.4689    
## half          1.669e+07  1.514e+07   1.102   0.2706    
## quarter       7.128e+06  7.701e+06   0.926   0.3549    
## month        -1.806e+07  3.711e+06  -4.866  1.4e-06 ***
## month.xts            NA         NA      NA       NA    
## month.lbl.L          NA         NA      NA       NA    
## month.lbl.Q   5.420e+06  3.451e+06   1.570   0.1167    
## month.lbl.C   6.025e+05  7.687e+06   0.078   0.9376    
## month.lbl^4  -2.337e+06  3.422e+06  -0.683   0.4947    
## month.lbl^5  -6.224e+06  8.735e+06  -0.713   0.4764    
## month.lbl^6   7.658e+06  3.455e+06   2.216   0.0270 *  
## month.lbl^7   6.488e+06  5.521e+06   1.175   0.2403    
## month.lbl^8   3.082e+06  3.397e+06   0.907   0.3645    
## month.lbl^9          NA         NA      NA       NA    
## month.lbl^10 -5.133e+06  3.389e+06  -1.515   0.1303    
## month.lbl^11         NA         NA      NA       NA    
## day                  NA         NA      NA       NA    
## hour                 NA         NA      NA       NA    
## minute               NA         NA      NA       NA    
## second               NA         NA      NA       NA    
## wday         -7.561e+05  1.391e+06  -0.544   0.5868    
## wday.xts             NA         NA      NA       NA    
## wday.lbl.L           NA         NA      NA       NA    
## wday.lbl.Q    2.538e+06  3.569e+06   0.711   0.4773    
## wday.lbl.C   -6.012e+06  2.570e+06  -2.339   0.0196 *  
## wday.lbl^4   -1.394e+06  2.210e+06  -0.631   0.5284    
## mday                 NA         NA      NA       NA    
## yday                 NA         NA      NA       NA    
## week          1.249e+05  3.900e+06   0.032   0.9745    
## week.iso      3.581e+05  2.458e+05   1.457   0.1456    
## week2        -2.302e+06  2.189e+06  -1.052   0.2932    
## week3         8.228e+05  1.233e+06   0.667   0.5047    
## week4         1.940e+06  9.881e+05   1.963   0.0500 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 26960000 on 731 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.2628,	Adjusted R-squared:  0.2396 
## F-statistic: 11.33 on 23 and 731 DF,  p-value: < 2.2e-16
{% endhighlight %}

Now we need to build the future data to model. We already have the index in the `actual_future` data. However, in practice we don't normally have the future index. Let's build it using the existing index. There's three steps. First, extract the index from the training set. Second, make a future index factoring in holidays and weekly inspection if necessary. Third, use the future index to create a time series signature that will be used for prediction in the next step.


{% highlight r %}
# US trading holidays in 2016
holidays <- c("2016-01-01", "2016-01-18", "2016-02-15", "2016-03-25", "2016-05-30",
              "2016-07-04", "2016-09-05", "2016-11-24", "2016-12-23", "2016-12-26",
              "2016-12-30") %>%
    ymd()
# Build new data for prediction: 3 Steps
new_data <- train %>%
    tk_index() %>%
    tk_make_future_timeseries(n_future = 252, skip_values = holidays, inspect_weekdays = TRUE) %>%
    tk_get_timeseries_signature()
# New data should look like this
new_data
{% endhighlight %}



{% highlight text %}
## # A tibble: 252 × 23
##         index  index.num   diff  year  half quarter month month.xts
##        <date>      <int>  <int> <int> <int>   <int> <int>     <int>
## 1  2016-01-04 1451865600     NA  2016     1       1     1         0
## 2  2016-01-05 1451952000  86400  2016     1       1     1         0
## 3  2016-01-06 1452038400  86400  2016     1       1     1         0
## 4  2016-01-07 1452124800  86400  2016     1       1     1         0
## 5  2016-01-08 1452211200  86400  2016     1       1     1         0
## 6  2016-01-11 1452470400 259200  2016     1       1     1         0
## 7  2016-01-12 1452556800  86400  2016     1       1     1         0
## 8  2016-01-13 1452643200  86400  2016     1       1     1         0
## 9  2016-01-14 1452729600  86400  2016     1       1     1         0
## 10 2016-01-15 1452816000  86400  2016     1       1     1         0
## # ... with 242 more rows, and 15 more variables: month.lbl <ord>,
## #   day <int>, hour <int>, minute <int>, second <int>, wday <int>,
## #   wday.xts <int>, wday.lbl <ord>, mday <int>, yday <int>,
## #   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>
{% endhighlight %}

Now use the `predict()` function to run a regression prediction on the new data.


{% highlight r %}
# Prediction using a linear model, fit_lm, on future index time series signature
pred_lm <- predict(fit_lm, newdata = new_data)
{% endhighlight %}

Let's compare the prediction to the actual daily FB volume in 2016. Using the `add_column()` function, we can add the predictions to the actual data, `actual_future`. We can then plot the prediction using `ggplot()`.


{% highlight r %}
# Add predicted values to actuals data
actual_future <- actual_future %>%
    add_column(yhat = pred_lm) 
# Plot using ggplot
actual_future %>%
    ggplot(aes(x = date)) +
    geom_line(aes(y = volume), data = train, color = palette_light()[[1]]) +
    geom_line(aes(y = volume), color = palette_light()[[1]]) +
    geom_line(aes(y = yhat), color = palette_light()[[2]]) +
    scale_y_continuous(labels = scales::comma) +
    labs(title = "Forecasting FB Daily Volume: New Methods Using Data Mining",
         subtitle = "Linear Regression Model Applied to Time Series Signature",
         x = "",
         y = "Volume",
         caption = "Data from Yahoo! Finance: 'FB' Daily Volume from 2013 to 2016.") +
    theme_tq(base_size = 12)
{% endhighlight %}

![plot of chunk unnamed-chunk-22](/figure/source/2017-5-2-timekit-0-2-0/unnamed-chunk-22-1.png)

The predictions are a bit off as compared to the actuals and in some months the values are actually negative which is impossible. While the result is not necessarily earth shattering, let's see how a regression algorithm performs data with a more prevalent pattern. Note that we did a performance comparison and the `prophet` package with default settings did much better job at identifying the volume pattern. With different modeling methods and tuning, the data mining approach can be significantly improved but it's difficult to tell if the performance would be better than `prophet`. 

## Example 4: Forecasting Alcohol Sales

In this example, we'll evaluate a time series with a more prevalent pattern. The beauty of this example is that you will see the power of data mining the time series signature with just a simple linear regression. We'll be using a linear regression model again to model the time series signature, but you should be thinking about what other better modeling methods could be implemented. The example is truncated for brevity since the major steps are the same as Example 3.


{% highlight r %}
# Collect data and convert to yearmon regularized monthly data
beer_sales <- tq_get("S4248SM144NCEN", get = "economic.data", from = "2000-01-01")
beer_sales <- beer_sales %>%
    mutate(date = as.yearmon(date))

# Build training set and actual_future for comparison
train <- beer_sales %>%
    filter(date < 2014)
actual_future <- beer_sales %>%
    filter(date >= 2014)

# Add time series signature to training set
train <- train %>%
    tk_augment_timeseries_signature()

# Fit the linear regression model to predict price variable
fit_lm <- lm(price ~ ., data = train[,-1])

# Create the new date following the 3 steps
new_data <- train %>%
    tk_index() %>%
    tk_make_future_timeseries(n_future = 38) %>%
    tk_get_timeseries_signature()

# Make predictions using predict
pred_lm <- predict(object = fit_lm, newdata = new_data)

# Add predictions to actual_future
actual_future <- actual_future %>%
    add_column(yhat = pred_lm) 

# Plot using ggplot
actual_future %>%
    ggplot(aes(x = date)) +
    geom_line(aes(y = price), data = train, color = palette_light()[[1]]) +
    geom_line(aes(y = price), color = palette_light()[[1]]) +
    geom_line(aes(y = yhat), color = palette_light()[[2]]) +
    scale_x_yearmon() +
    scale_y_continuous(labels = scales::comma) +
    labs(title = "Forecasting Alcohol Sales Using Data Mining",
         subtitle = "Linear Regression Model Applied to Time Series Signature",
         x = "",
         y = "USD (Millions)",
         caption = "Data from FRED Code 'S4248SM144NCEN' 2000 to present.") +
    theme_tq(base_size = 12)
{% endhighlight %}

![plot of chunk unnamed-chunk-23](/figure/source/2017-5-2-timekit-0-2-0/unnamed-chunk-23-1.png)

When a pattern is present, data mining using the time series signature can provide exceptional results. Further, the analyst has the flexibility to implement other data mining techniques and methods. We implemented a linear regression, but possibly other regression methods would work better. 

## Example 5: Simplified and Extensible Coercion

In the final example, we'll examine briefly the various coercion functions that enable simplified coercion back and forth. We'll start with the `FB_tbl` data.


{% highlight r %}
FB_tbl
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,008 × 2
##          date    volume
##        <date>     <dbl>
## 1  2013-01-02  69846400
## 2  2013-01-03  63140600
## 3  2013-01-04  72715400
## 4  2013-01-07  83781800
## 5  2013-01-08  45871300
## 6  2013-01-09 104787700
## 7  2013-01-10  95316400
## 8  2013-01-11  89598000
## 9  2013-01-14  98892800
## 10 2013-01-15 173242600
## # ... with 998 more rows
{% endhighlight %}

We use the various `timekit` coercion methods to go back and forth without data loss. See how the original tibble is returned. Note the argument `silent = TRUE` removes the warning that the "date" column is being dropped. This is desirable since `xts` and the other matrix-based time classes should only use numeric data.


{% highlight r %}
FB_tbl %>%
    tk_xts(silent = TRUE) %>%            # Coerce to xts
    tk_zoo() %>%                         # Coerce to zoo
    tk_ts(start = 2013, freq = 252) %>%  # Coerce to ts
    tk_xts() %>%                         # Coerce back to xts
    tk_tbl()                             # Coerce back to tbl
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,008 × 2
##         index    volume
##        <date>     <dbl>
## 1  2013-01-02  69846400
## 2  2013-01-03  63140600
## 3  2013-01-04  72715400
## 4  2013-01-07  83781800
## 5  2013-01-08  45871300
## 6  2013-01-09 104787700
## 7  2013-01-10  95316400
## 8  2013-01-11  89598000
## 9  2013-01-14  98892800
## 10 2013-01-15 173242600
## # ... with 998 more rows
{% endhighlight %}

One caveat is the going from `ts` to `tbl`. The default is `timekit_idx = FALSE` argument which returns a regularized index. If the time-based index is needed, just set `timekit_idx = TRUE`.


{% highlight r %}
FB_tbl %>%
    tk_ts(start = 2013, freq = 252, silent = TRUE) %>%
    tk_tbl(timekit_idx = TRUE)
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,008 × 2
##         index    volume
##        <date>     <dbl>
## 1  2013-01-02  69846400
## 2  2013-01-03  63140600
## 3  2013-01-04  72715400
## 4  2013-01-07  83781800
## 5  2013-01-08  45871300
## 6  2013-01-09 104787700
## 7  2013-01-10  95316400
## 8  2013-01-11  89598000
## 9  2013-01-14  98892800
## 10 2013-01-15 173242600
## # ... with 998 more rows
{% endhighlight %}





# Recap

This was a lengthy post, but hopefully you can now see how `timekit` benefits time series analysis. We reviewed several of the functions related to extracting an index, adding a time series signature to an index or augmenting to a data frame, using the time series summary to understand the features including periodicity and regularity, making a future time series that accounts for weekends and holidays, and coercing between various time series object classes. We also saw how the time series signature can be used in predictive analytics and data mining. The goal was to introduce you to `timekit`. Hopefully you now have a baseline to assist with future time series analysis. 

# Announcements

If you're interested in meeting with the members of [_Business Science_](http://www.business-science.io/), we'll be speaking at the following upcoming conferences:

* [R/Finance: Chicago, May 19-20](http://www.rinfinance.com/)
* [Enterprise Applications of the R Language (EARL): San Francisco, June 5-7](https://earlconf.com/)


# Important Links

If you are interested learning more about `timekit`:

* [Visit our `pkgdown` site for detailed documentation](https://business-science.github.io/timekit/)
* [Visit our GitHub site for code updates](https://github.com/business-science/timekit)
* [Visit our website for news and announcements](http://www.business-science.io/)

# Further Reading

I find the [R Data Mining Website](http://www.rdatamining.com/) and Reference Card to be an invaluable tool when researching (and trying to remember) the various data mining techniques. Many of these techniques can be implemented in time series analysis with `timekit`.


